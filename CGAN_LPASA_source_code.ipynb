{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "44smJ4Q1fotI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohsLPAj2fAt7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg19\n",
        "from PIL import Image\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr, structural_similarity as compare_ssim\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set memory management configuration\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Learnable Self-Attention Module\n",
        "class LearnableSelfAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super(LearnableSelfAttention, self).__init__()\n",
        "        self.query = nn.Conv2d(channels, channels // reduction, kernel_size=1)\n",
        "        self.key = nn.Conv2d(channels, channels // reduction, kernel_size=1)\n",
        "        self.value = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "        self.scale = nn.Parameter(torch.tensor(0.1))  # Learnable scaling parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, height, width = x.size()\n",
        "\n",
        "        # Compute query, key, and value\n",
        "        query = self.query(x).view(batch, -1, height * width).permute(0, 2, 1)\n",
        "        key = self.key(x).view(batch, -1, height * width)\n",
        "        value = self.value(x).view(batch, -1, height * width).permute(0, 2, 1)\n",
        "\n",
        "        # Compute attention\n",
        "        attention = torch.softmax(torch.bmm(query, key) / (channels ** 0.5), dim=-1)\n",
        "        out = torch.bmm(attention, value).permute(0, 2, 1).view(batch, channels, height, width)\n",
        "\n",
        "        return self.scale * out + x  # Residual connection\n",
        "\n",
        "\n",
        "# Dense Residual Block\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(DenseResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        return x + out  # Residual connection\n",
        "\n",
        "\n",
        "# Enhanced Generator with Dense Residual Blocks and Learnable Self-Attention\n",
        "class AdvancedAttentionGenerator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(AdvancedAttentionGenerator, self).__init__()\n",
        "        self.encoder1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=3, padding=1), nn.ReLU())\n",
        "        self.encoder2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "        self.encoder3 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "\n",
        "        # Latent block with dense residuals and self-attention\n",
        "        self.latent = nn.Sequential(\n",
        "            DenseResidualBlock(256),\n",
        "            LearnableSelfAttention(256),\n",
        "            DenseResidualBlock(256)\n",
        "        )\n",
        "\n",
        "        self.decoder1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder3 = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.encoder1(x)  # 256x256 -> 256x256\n",
        "        e2 = self.encoder2(e1)  # 256x256 -> 128x128\n",
        "        e3 = self.encoder3(e2)  # 128x128 -> 64x64\n",
        "\n",
        "        latent = self.latent(e3)  # 64x64 -> 64x64\n",
        "\n",
        "        d1 = self.decoder1(latent) + e2  # Skip connection\n",
        "        d2 = self.decoder2(d1) + e1  # Skip connection\n",
        "        return self.decoder3(d2)  # 256x256 -> 256x256\n",
        "\n",
        "\n",
        "# Lightweight Discriminator\n",
        "class LightweightDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc):\n",
        "        super(LightweightDiscriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Dataset Class\n",
        "class HazyDataset(Dataset):\n",
        "    def __init__(self, hazy_dir, clear_dir, transform=None):\n",
        "        self.hazy_paths = sorted([os.path.join(hazy_dir, f) for f in os.listdir(hazy_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        self.clear_paths = sorted([os.path.join(clear_dir, f) for f in os.listdir(clear_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "        if len(self.hazy_paths) != len(self.clear_paths):\n",
        "            raise ValueError(f\"Mismatch in number of hazy ({len(self.hazy_paths)}) and clear ({len(self.clear_paths)}) images. Please check the dataset.\")\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hazy_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hazy = Image.open(self.hazy_paths[idx]).convert(\"RGB\")\n",
        "        clear = Image.open(self.clear_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            hazy = self.transform(hazy)\n",
        "            clear = self.transform(clear)\n",
        "        return hazy, clear\n",
        "def hinge_loss_discriminator(real_output, fake_output):\n",
        "    # Hinge loss for discriminator\n",
        "    real_loss = torch.mean(torch.clamp(1 - real_output, min=0))\n",
        "    fake_loss = torch.mean(torch.clamp(1 + fake_output, min=0))\n",
        "\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def hinge_loss_generator(fake_output):\n",
        "    # Hinge loss for generator\n",
        "    return -torch.mean(fake_output)\n",
        "\n",
        "# Loss Functions\n",
        "def perceptual_loss(output, target, vgg):\n",
        "    output_features = vgg(output)\n",
        "    target_features = vgg(target)\n",
        "    return nn.L1Loss()(output_features, target_features)\n",
        "\n",
        "# def hinge_loss(output, target):\n",
        "#     # replacing 0 = -1\n",
        "#     new_predicted = np.array([-1 if i==0 else i for i in output])\n",
        "\n",
        "#     # calculating hinge loss\n",
        "#     hinge_loss = np.mean([max(0, 1-x*y) for x, y in zip(target, new_predicted)])\n",
        "    return hinge_loss\n",
        "\n",
        "class SSIMLoss(nn.Module):\n",
        "    def forward(self, x, y):\n",
        "        x = x.detach().cpu().numpy()\n",
        "        y = y.detach().cpu().numpy()\n",
        "\n",
        "        # Ensure input shape is (H, W, C) for SSIM\n",
        "        x = np.transpose(x, (0, 2, 3, 1))  # Convert (B, C, H, W) → (B, H, W, C)\n",
        "        y = np.transpose(y, (0, 2, 3, 1))\n",
        "\n",
        "        # Compute SSIM for each image in the batch\n",
        "        ssim_values = [compare_ssim(x[i], y[i], data_range=1.0, win_size=3, channel_axis=-1) for i in range(x.shape[0])]\n",
        "\n",
        "        return 1 - np.mean(ssim_values)  # Return SSIM loss\n",
        "\n",
        "\n",
        "def entropy_regularization(generator_output):\n",
        "    probabilities = torch.sigmoid(generator_output)  # Convert to probability distribution\n",
        "    return -torch.mean(probabilities * torch.log(probabilities + 1e-8) +\n",
        "                       (1 - probabilities) * torch.log(1 - probabilities + 1e-8))\n",
        "\n",
        "def compute_total_loss(generator_output, target, discriminator, hazy, vgg, fake_output1):\n",
        "    # Content Loss (L1)\n",
        "    content_loss = nn.L1Loss()(generator_output, target)\n",
        "\n",
        "    # Perceptual Loss (VGG Features)\n",
        "    perceptual_loss = nn.L1Loss()(vgg(generator_output), vgg(target))\n",
        "\n",
        "    # Discriminator outputs\n",
        "    real_pair = torch.cat([hazy, target], dim=1)\n",
        "    fake_pair = torch.cat([hazy, generator_output], dim=1)\n",
        "\n",
        "    real_logits = discriminator(real_pair)\n",
        "    fake_logits = discriminator(fake_pair)\n",
        "    fake_output = discriminator(torch.cat([target, generator_output], dim=1))\n",
        "\n",
        "    d_loss = hinge_loss_discriminator(fake_output1, fake_output)\n",
        "    g_loss1 = hinge_loss_generator(fake_output)\n",
        "\n",
        "    # Relativistic GAN Loss\n",
        "    d_real_loss = nn.BCEWithLogitsLoss()(real_logits - torch.mean(fake_logits), torch.ones_like(real_logits))\n",
        "    d_fake_loss = nn.BCEWithLogitsLoss()(fake_logits - torch.mean(real_logits), torch.zeros_like(fake_logits))\n",
        "    adversarial_loss = (d_real_loss + d_fake_loss) / 2  # Balanced GAN loss\n",
        "\n",
        "    # Structural Similarity Loss (SSIM)\n",
        "    ssim_loss = SSIMLoss()(generator_output, target)\n",
        "\n",
        "    # Entropy Regularization Loss\n",
        "    entropy_loss = entropy_regularization(generator_output)\n",
        "\n",
        "    # Final Weighted Loss Combination\n",
        "    total_loss = (\n",
        "        120 * content_loss +  # Slightly reduced for better perceptual learning\n",
        "        100 * perceptual_loss +  # Increased to improve feature retention\n",
        "        70 * ssim_loss +  # Increased to emphasize texture and fine details\n",
        "        1.0 * entropy_loss +  # Increased for better regularization\n",
        "        d_loss + g_loss1  # Adversarial loss remains unchanged\n",
        "    )\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(generator, discriminator, train_loader, val_loader, vgg, num_epochs, checkpoint_dir, patience):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "    vgg.to(device).eval()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    optimizer_G = optim.AdamW(generator.parameters(), lr=5e-5)\n",
        "    optimizer_D = optim.AdamW(discriminator.parameters(), lr=1e-4)\n",
        "    best_psnr = 0\n",
        "    early_stop_count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        generator.train()\n",
        "        for hazy, clear in train_loader:\n",
        "            hazy, clear = hazy.to(device), clear.to(device)\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                dehazed = generator(hazy)\n",
        "                fake_output = discriminator(torch.cat([hazy, dehazed], dim=1))\n",
        "                total_loss = compute_total_loss(dehazed, clear, discriminator, hazy, vgg,fake_output)  # Updated loss function\n",
        "\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.step(optimizer_G)\n",
        "            scaler.update()\n",
        "\n",
        "        # Validation phase\n",
        "        # Validation phase\n",
        "        # Validation phase\n",
        "        generator.eval()\n",
        "        val_psnr, val_ssim = [], []\n",
        "        with torch.no_grad():\n",
        "            for hazy, clear in val_loader:\n",
        "                hazy, clear = hazy.to(device), clear.to(device)\n",
        "                dehazed = generator(hazy)\n",
        "\n",
        "                # Convert tensors to numpy arrays\n",
        "                clear_np = clear.squeeze(0).cpu().numpy()  # Ensure (C, H, W) shape\n",
        "                dehazed_np = dehazed.squeeze(0).cpu().numpy()  # Ensure (C, H, W) shape\n",
        "\n",
        "                # Ensure array has at least 3 dimensions before transposing\n",
        "                if clear_np.ndim == 3:\n",
        "                    clear_np = np.transpose(clear_np, (1, 2, 0))  # Convert (C, H, W) → (H, W, C)\n",
        "                    dehazed_np = np.transpose(dehazed_np, (1, 2, 0))  # Convert (C, H, W) → (H, W, C)\n",
        "\n",
        "                # Compute PSNR\n",
        "                psnr = compare_psnr(clear_np, dehazed_np, data_range=1.0)\n",
        "\n",
        "                # Compute SSIM\n",
        "                # Compute SSIM with adaptive window size\n",
        "                min_dim = min(clear_np.shape[0], clear_np.shape[1])  # Find the smallest image dimension\n",
        "                win_size = min(7, min_dim)  # Ensure window size does not exceed image size\n",
        "\n",
        "                ssim = compare_ssim(clear_np, dehazed_np, data_range=1.0, channel_axis=-1, win_size=win_size)\n",
        "\n",
        "\n",
        "                val_psnr.append(psnr)\n",
        "                val_ssim.append(ssim)\n",
        "\n",
        "        avg_psnr, avg_ssim = np.mean(val_psnr), np.mean(val_ssim)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "        if avg_psnr > best_psnr:\n",
        "            best_psnr = avg_psnr\n",
        "            early_stop_count = 0\n",
        "            torch.save(generator.state_dict(), os.path.join(checkpoint_dir, \"best_generator.pth\"))\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "            if early_stop_count >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                return\n",
        "\n",
        "\n",
        "# Paths\n",
        "train_hazy_dir = \"train hazy folder path\"\n",
        "train_clear_dir = \"train clear folder path\"\n",
        "val_hazy_dir =\"validation hazy folder path\"\n",
        "val_clear_dir =  \"validation clear folder path\"\n",
        "checkpoint_dir = \"checkpoint directory\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_dataset = HazyDataset(train_hazy_dir, train_clear_dir, transform=transform)\n",
        "val_dataset = HazyDataset(val_hazy_dir, val_clear_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Models\n",
        "generator = AdvancedAttentionGenerator(3, 3)\n",
        "discriminator = LightweightDiscriminator(6)\n",
        "vgg = vgg19(weights=\"IMAGENET1K_V1\").features[:16].eval()\n",
        "#vgg = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
        "\n",
        "# Train\n",
        "train(generator, discriminator, train_loader, val_loader, vgg, num_epochs=500, checkpoint_dir=checkpoint_dir, patience=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "uvJtyQfPftYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time  # For measuring runtime\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr, structural_similarity as compare_ssim\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Paths for testing set (O-Haze or NH-Haze Dataset)\n",
        "test_hazy_path = \"test hazy folder path\"\n",
        "test_clear_path = \"test clear folder path\"\n",
        "save_dehazed_path = \"folder path for saving dehazed images\"\n",
        "\n",
        "if not os.path.exists(save_dehazed_path):\n",
        "    os.makedirs(save_dehazed_path)\n",
        "\n",
        "# Helper function to load and preprocess images\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    original_size = image.size  # Store original size (width, height)\n",
        "    image = image.resize((256, 256), Image.Resampling.LANCZOS)  # Resize to 256x256 for model input\n",
        "    image_tensor = ToTensor()(image).unsqueeze(0)  # Convert to tensor and add batch dim\n",
        "    return image_tensor, original_size\n",
        "\n",
        "# Helper function to save upsampled dehazed images\n",
        "def save_image(image_tensor, save_path, original_size):\n",
        "    image_np = image_tensor.squeeze(0).cpu().numpy().transpose((1, 2, 0))\n",
        "    image_np = np.clip(image_np, 0, 1) * 255.0\n",
        "    image_pil = Image.fromarray(image_np.astype('uint8'))\n",
        "    image_pil = image_pil.resize(original_size, Image.Resampling.LANCZOS)  # Upsample to original size\n",
        "    image_pil.save(save_path)\n",
        "\n",
        "# Dataset Loader for Testing (aligning with training dataset loader)\n",
        "class SOTSTestDataset:\n",
        "    def __init__(self, hazy_dir, clear_dir):\n",
        "        self.hazy_paths = sorted([os.path.join(hazy_dir, f) for f in os.listdir(hazy_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        self.clear_paths = sorted([os.path.join(clear_dir, f) for f in os.listdir(clear_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "        if len(self.hazy_paths) != len(self.clear_paths):\n",
        "            raise ValueError(f\"Mismatch in number of hazy ({len(self.hazy_paths)}) and clear ({len(self.clear_paths)}) images. Please check the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hazy_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.hazy_paths[idx], self.clear_paths[idx]\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = SOTSTestDataset(test_hazy_path, test_clear_path)\n",
        "\n",
        "# Initialize model and load the best checkpoint\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = AdvancedAttentionGenerator(3, 3).to(device)\n",
        "\n",
        "# Load model checkpoint safely\n",
        "checkpoint_path = \"/content/best_generator.pth\"\n",
        "generator.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n",
        "generator.eval()\n",
        "\n",
        "# Initialize metrics and runtime tracker\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "runtimes = []  # List to store runtime for each image\n",
        "\n",
        "# Testing loop\n",
        "for idx in range(len(test_dataset)):\n",
        "    hazy_path, clear_path = test_dataset[idx]\n",
        "\n",
        "    # Load hazy and clear images\n",
        "    hazy_tensor, original_size = load_image(hazy_path)\n",
        "    clear_tensor, _ = load_image(clear_path)\n",
        "\n",
        "    hazy_tensor = hazy_tensor.to(device)\n",
        "    clear_tensor = clear_tensor.to(device)\n",
        "\n",
        "    # Measure runtime\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generate dehazed image\n",
        "    with torch.no_grad():\n",
        "        dehazed_tensor = generator(hazy_tensor)\n",
        "\n",
        "    end_time = time.time()\n",
        "    runtimes.append(end_time - start_time)  # Record runtime\n",
        "\n",
        "    # Save dehazed image resized to the original size\n",
        "    hazy_name = os.path.basename(hazy_path)\n",
        "    save_path = os.path.join(save_dehazed_path, hazy_name)\n",
        "    save_image(dehazed_tensor, save_path, original_size)\n",
        "\n",
        "    # Resize clear and dehazed images to the original size for metric computation\n",
        "    clear_resized = F.interpolate(clear_tensor, size=(original_size[1], original_size[0]), mode='bilinear', align_corners=False)\n",
        "    dehazed_resized = F.interpolate(dehazed_tensor, size=(original_size[1], original_size[0]), mode='bilinear', align_corners=False)\n",
        "\n",
        "    # Convert tensors to numpy arrays for metric computation (Ensure correct format)\n",
        "    clear_np = clear_resized.squeeze(0).cpu().numpy().transpose((1, 2, 0))  # Convert to (H, W, C)\n",
        "    dehazed_np = dehazed_resized.squeeze(0).cpu().numpy().transpose((1, 2, 0))  # Convert to (H, W, C)\n",
        "\n",
        "    # Compute PSNR (Data Range should be 1.0, NOT 255, since tensors are normalized)\n",
        "    psnr = compare_psnr(clear_np, dehazed_np, data_range=1.0)\n",
        "\n",
        "    # Compute SSIM using multi-channel support\n",
        "    ssim = compare_ssim(clear_np, dehazed_np, data_range=1.0, channel_axis=-1)\n",
        "\n",
        "    # Store results\n",
        "    psnr_list.append(psnr)\n",
        "    ssim_list.append(ssim)\n",
        "\n",
        "# Compute average PSNR, SSIM, and runtime\n",
        "avg_psnr = np.mean(psnr_list)\n",
        "avg_ssim = np.mean(ssim_list)\n",
        "max_psnr = np.max(psnr_list)\n",
        "max_ssim = np.max(ssim_list)\n",
        "avg_runtime = np.mean(runtimes)\n",
        "\n",
        "# Print results\n",
        "print(f\"Average PSNR: {avg_psnr:.4f}\")\n",
        "print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
        "print(f\"Max PSNR: {max_psnr:.4f}\")\n",
        "print(f\"Max SSIM: {max_ssim:.4f}\")\n",
        "print(f\"Average Runtime per Image: {avg_runtime:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "onZEPcnefvRz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}