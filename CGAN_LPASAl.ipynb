{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB7qtl26QIk3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg19\n",
        "from PIL import Image\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr, structural_similarity as compare_ssim\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set memory management configuration\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Learnable Self-Attention Module\n",
        "class LearnableSelfAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super(LearnableSelfAttention, self).__init__()\n",
        "        self.query = nn.Conv2d(channels, channels // reduction, kernel_size=1)\n",
        "        self.key = nn.Conv2d(channels, channels // reduction, kernel_size=1)\n",
        "        self.value = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "        self.scale = nn.Parameter(torch.tensor(0.1))  # Learnable scaling parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, height, width = x.size()\n",
        "\n",
        "        # Compute query, key, and value\n",
        "        query = self.query(x).view(batch, -1, height * width).permute(0, 2, 1)\n",
        "        key = self.key(x).view(batch, -1, height * width)\n",
        "        value = self.value(x).view(batch, -1, height * width).permute(0, 2, 1)\n",
        "\n",
        "        # Compute attention\n",
        "        attention = torch.softmax(torch.bmm(query, key) / (channels ** 0.5), dim=-1)\n",
        "        out = torch.bmm(attention, value).permute(0, 2, 1).view(batch, channels, height, width)\n",
        "\n",
        "        return self.scale * out + x  # Residual connection\n",
        "\n",
        "\n",
        "# Dense Residual Block\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(DenseResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        return x + out  # Residual connection\n",
        "\n",
        "\n",
        "# Enhanced Generator with Dense Residual Blocks and Learnable Self-Attention\n",
        "class AdvancedAttentionGenerator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(AdvancedAttentionGenerator, self).__init__()\n",
        "        self.encoder1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=3, padding=1), nn.ReLU())\n",
        "        self.encoder2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "        self.encoder3 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), nn.ReLU())\n",
        "\n",
        "        # Latent block with dense residuals and self-attention\n",
        "        self.latent = nn.Sequential(\n",
        "            DenseResidualBlock(256),\n",
        "            LearnableSelfAttention(256),\n",
        "            DenseResidualBlock(256)\n",
        "        )\n",
        "\n",
        "        self.decoder1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder3 = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.encoder1(x)  # 256x256 -> 256x256\n",
        "        e2 = self.encoder2(e1)  # 256x256 -> 128x128\n",
        "        e3 = self.encoder3(e2)  # 128x128 -> 64x64\n",
        "\n",
        "        latent = self.latent(e3)  # 64x64 -> 64x64\n",
        "\n",
        "        d1 = self.decoder1(latent) + e2  # Skip connection\n",
        "        d2 = self.decoder2(d1) + e1  # Skip connection\n",
        "        return self.decoder3(d2)  # 256x256 -> 256x256\n",
        "\n",
        "\n",
        "# Lightweight Discriminator\n",
        "class LightweightDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc):\n",
        "        super(LightweightDiscriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Dataset Class\n",
        "class HazyDataset(Dataset):\n",
        "    def __init__(self, hazy_dir, clear_dir, transform=None):\n",
        "        self.hazy_paths = sorted([os.path.join(hazy_dir, f) for f in os.listdir(hazy_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        self.clear_paths = sorted([os.path.join(clear_dir, f) for f in os.listdir(clear_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "        if len(self.hazy_paths) != len(self.clear_paths):\n",
        "            raise ValueError(f\"Mismatch in number of hazy ({len(self.hazy_paths)}) and clear ({len(self.clear_paths)}) images. Please check the dataset.\")\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hazy_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hazy = Image.open(self.hazy_paths[idx]).convert(\"RGB\")\n",
        "        clear = Image.open(self.clear_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            hazy = self.transform(hazy)\n",
        "            clear = self.transform(clear)\n",
        "        return hazy, clear\n",
        "def hinge_loss_discriminator(real_output, fake_output):\n",
        "    # Hinge loss for discriminator\n",
        "    real_loss = torch.mean(torch.clamp(1 - real_output, min=0))\n",
        "    fake_loss = torch.mean(torch.clamp(1 + fake_output, min=0))\n",
        "\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def hinge_loss_generator(fake_output):\n",
        "    # Hinge loss for generator\n",
        "    return -torch.mean(fake_output)\n",
        "\n",
        "# Loss Functions\n",
        "def perceptual_loss(output, target, vgg):\n",
        "    output_features = vgg(output)\n",
        "    target_features = vgg(target)\n",
        "    return nn.L1Loss()(output_features, target_features)\n",
        "\n",
        "# def hinge_loss(output, target):\n",
        "#     # replacing 0 = -1\n",
        "#     new_predicted = np.array([-1 if i==0 else i for i in output])\n",
        "\n",
        "#     # calculating hinge loss\n",
        "#     hinge_loss = np.mean([max(0, 1-x*y) for x, y in zip(target, new_predicted)])\n",
        "    return hinge_loss\n",
        "\n",
        "class SSIMLoss(nn.Module):\n",
        "    def forward(self, x, y):\n",
        "        x = x.detach().cpu().numpy()\n",
        "        y = y.detach().cpu().numpy()\n",
        "\n",
        "        # Ensure input shape is (H, W, C) for SSIM\n",
        "        x = np.transpose(x, (0, 2, 3, 1))  # Convert (B, C, H, W) â†’ (B, H, W, C)\n",
        "        y = np.transpose(y, (0, 2, 3, 1))\n",
        "\n",
        "        # Compute SSIM for each image in the batch\n",
        "        ssim_values = [compare_ssim(x[i], y[i], data_range=1.0, win_size=3, channel_axis=-1) for i in range(x.shape[0])]\n",
        "\n",
        "        return 1 - np.mean(ssim_values)  # Return SSIM loss\n",
        "\n",
        "\n",
        "def entropy_regularization(generator_output):\n",
        "    probabilities = torch.sigmoid(generator_output)  # Convert to probability distribution\n",
        "    return -torch.mean(probabilities * torch.log(probabilities + 1e-8) +\n",
        "                       (1 - probabilities) * torch.log(1 - probabilities + 1e-8))\n",
        "\n",
        "def compute_total_loss(generator_output, target, discriminator, hazy, vgg, fake_output1):\n",
        "    # Content Loss (L1)\n",
        "    content_loss = nn.L1Loss()(generator_output, target)\n",
        "\n",
        "    # Perceptual Loss (VGG Features)\n",
        "    perceptual_loss = nn.L1Loss()(vgg(generator_output), vgg(target))\n",
        "\n",
        "    # Discriminator outputs\n",
        "    real_pair = torch.cat([hazy, target], dim=1)\n",
        "    fake_pair = torch.cat([hazy, generator_output], dim=1)\n",
        "\n",
        "    real_logits = discriminator(real_pair)\n",
        "    fake_logits = discriminator(fake_pair)\n",
        "    fake_output = discriminator(torch.cat([target, generator_output], dim=1))\n",
        "\n",
        "    d_loss = hinge_loss_discriminator(fake_output1, fake_output)\n",
        "    g_loss1 = hinge_loss_generator(fake_output)\n",
        "\n",
        "    # Relativistic GAN Loss\n",
        "    d_real_loss = nn.BCEWithLogitsLoss()(real_logits - torch.mean(fake_logits), torch.ones_like(real_logits))\n",
        "    d_fake_loss = nn.BCEWithLogitsLoss()(fake_logits - torch.mean(real_logits), torch.zeros_like(fake_logits))\n",
        "    adversarial_loss = (d_real_loss + d_fake_loss) / 2  # Balanced GAN loss\n",
        "\n",
        "    # Structural Similarity Loss (SSIM)\n",
        "    ssim_loss = SSIMLoss()(generator_output, target)\n",
        "\n",
        "    # Entropy Regularization Loss\n",
        "    entropy_loss = entropy_regularization(generator_output)\n",
        "\n",
        "    # Final Weighted Loss Combination\n",
        "    total_loss = (\n",
        "        120 * content_loss +  # Slightly reduced for better perceptual learning\n",
        "        100 * perceptual_loss +  # Increased to improve feature retention\n",
        "        70 * ssim_loss +  # Increased to emphasize texture and fine details\n",
        "        1.0 * entropy_loss +  # Increased for better regularization\n",
        "        d_loss + g_loss1  # Adversarial loss remains unchanged\n",
        "    )\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(generator, discriminator, train_loader, val_loader, vgg, num_epochs, checkpoint_dir, patience):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "    vgg.to(device).eval()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    optimizer_G = optim.AdamW(generator.parameters(), lr=5e-5)\n",
        "    optimizer_D = optim.AdamW(discriminator.parameters(), lr=1e-4)\n",
        "    best_psnr = 0\n",
        "    early_stop_count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        generator.train()\n",
        "        for hazy, clear in train_loader:\n",
        "            hazy, clear = hazy.to(device), clear.to(device)\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                dehazed = generator(hazy)\n",
        "                fake_output = discriminator(torch.cat([hazy, dehazed], dim=1))\n",
        "                total_loss = compute_total_loss(dehazed, clear, discriminator, hazy, vgg,fake_output)  # Updated loss function\n",
        "\n",
        "            scaler.scale(total_loss).backward()\n",
        "            scaler.step(optimizer_G)\n",
        "            scaler.update()\n",
        "\n",
        "        # Validation phase\n",
        "        # Validation phase\n",
        "        # Validation phase\n",
        "        generator.eval()\n",
        "        val_psnr, val_ssim = [], []\n",
        "        with torch.no_grad():\n",
        "            for hazy, clear in val_loader:\n",
        "                hazy, clear = hazy.to(device), clear.to(device)\n",
        "                dehazed = generator(hazy)\n",
        "\n",
        "                # Convert tensors to numpy arrays\n",
        "                clear_np = clear.squeeze(0).cpu().numpy()  # Ensure (C, H, W) shape\n",
        "                dehazed_np = dehazed.squeeze(0).cpu().numpy()  # Ensure (C, H, W) shape\n",
        "\n",
        "                # Ensure array has at least 3 dimensions before transposing\n",
        "                if clear_np.ndim == 3:\n",
        "                    clear_np = np.transpose(clear_np, (1, 2, 0))  # Convert (C, H, W) â†’ (H, W, C)\n",
        "                    dehazed_np = np.transpose(dehazed_np, (1, 2, 0))  # Convert (C, H, W) â†’ (H, W, C)\n",
        "\n",
        "                # Compute PSNR\n",
        "                psnr = compare_psnr(clear_np, dehazed_np, data_range=1.0)\n",
        "\n",
        "                # Compute SSIM\n",
        "                # Compute SSIM with adaptive window size\n",
        "                min_dim = min(clear_np.shape[0], clear_np.shape[1])  # Find the smallest image dimension\n",
        "                win_size = min(7, min_dim)  # Ensure window size does not exceed image size\n",
        "\n",
        "                ssim = compare_ssim(clear_np, dehazed_np, data_range=1.0, channel_axis=-1, win_size=win_size)\n",
        "\n",
        "\n",
        "                val_psnr.append(psnr)\n",
        "                val_ssim.append(ssim)\n",
        "\n",
        "        avg_psnr, avg_ssim = np.mean(val_psnr), np.mean(val_ssim)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "        if avg_psnr > best_psnr:\n",
        "            best_psnr = avg_psnr\n",
        "            early_stop_count = 0\n",
        "            torch.save(generator.state_dict(), os.path.join(checkpoint_dir, \"best_generator.pth\"))\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "            if early_stop_count >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                return\n",
        "\n",
        "\n",
        "# Paths\n",
        "train_hazy_dir = \"/content/drive/MyDrive/Updated_Code/NH_Dataset/Dataset/train/hazy\"\n",
        "train_clear_dir = \"/content/drive/MyDrive/Updated_Code/NH_Dataset/Dataset/train/label\"\n",
        "val_hazy_dir =\"/content/drive/MyDrive/Updated_Code/NH_Dataset/Dataset/test/hazy\"\n",
        "val_clear_dir =  \"/content/drive/MyDrive/Updated_Code/NH_Dataset/Dataset/test/label\"\n",
        "checkpoint_dir = \"/content/drive/MyDrive/Updated_Code/NH_Dataset/28thjuly_2025\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_dataset = HazyDataset(train_hazy_dir, train_clear_dir, transform=transform)\n",
        "val_dataset = HazyDataset(val_hazy_dir, val_clear_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Models\n",
        "generator = AdvancedAttentionGenerator(3, 3)\n",
        "discriminator = LightweightDiscriminator(6)\n",
        "vgg = vgg19(weights=\"IMAGENET1K_V1\").features[:16].eval()\n",
        "#vgg = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
        "\n",
        "# Train\n",
        "train(generator, discriminator, train_loader, val_loader, vgg, num_epochs=500, checkpoint_dir=checkpoint_dir, patience=50)\n"
      ]
    }
  ]
}